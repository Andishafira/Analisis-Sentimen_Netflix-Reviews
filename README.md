# üìä Analisis Sentimen Ulasan Netflix dengan Machine Learning & IndoBERT

Repositori ini berisi implementasi proyek *end-to-end* untuk analisis sentimen pada ulasan aplikasi Netflix yang diambil dari Google Play Store. Proyek ini mencakup seluruh alur kerja, mulai dari *web scraping* data, *preprocessing* teks, hingga melatih dan membandingkan beberapa model *machine learning* dan *deep learning* untuk klasifikasi sentimen (Positif/Negatif).

---

## üéØ **Tujuan Proyek**

* **Mengumpulkan Dataset**: Melakukan *scraping* untuk membangun dataset ulasan aplikasi Netflix berbahasa Indonesia secara mandiri.
* **Membangun Model Klasifikasi**: Mengembangkan model yang mampu mengklasifikasikan sentimen ulasan secara otomatis.
* **Eksperimen Model**: Membandingkan performa antara model *machine learning* klasik (LinearSVC, Logistic Regression) dengan model *deep learning* berbasis Transformer (IndoBERT).
* **Mencapai Target Akurasi**: Memastikan model final memiliki akurasi di atas **85%** pada *testing set*.

---

## üõ†Ô∏è **Alur Kerja Proyek**

Proyek ini dijalankan melalui beberapa tahapan utama yang sistematis:

### 1. Pengumpulan Data
* Data ulasan di-scraping dari Google Play Store menggunakan *library* `google-play-scraper`.
* Berhasil mengumpulkan **63.000 ulasan** berbahasa Indonesia.
* Dataset mentah disimpan dalam format `.csv` untuk diproses lebih lanjut.

### 2. Preprocessing & Pelabelan Data
* **Pelabelan Sentimen**: Ulasan diberi label berdasarkan skor rating:
    * `Positif`: Rating 4 atau 5.
    * `Negatif`: Rating 1, 2, atau 3.
* **Penanganan Kelas Tidak Seimbang**: Diterapkan teknik **undersampling** untuk menyeimbangkan jumlah data positif dan negatif, menghasilkan dataset final sebanyak **52.458 ulasan** yang seimbang.

### 3. Ekstraksi Fitur & Pembersihan Teks
Teks ulasan dibersihkan melalui beberapa tahap untuk persiapan pemodelan:
* **Case Folding**: Mengubah teks menjadi huruf kecil.
* **Pembersihan Karakter**: Menghapus angka, tanda baca, dan simbol.
* **Stopword Removal**: Menghapus kata-kata umum (seperti "yang", "di") menggunakan `NLTK` dan kamus *custom*.
* **Stemming**: Mengubah kata menjadi bentuk dasar dengan *library* `Sastrawi`.
* **Vektorisasi TF-IDF**: Teks yang bersih diubah menjadi vektor numerik menggunakan `TfidfVectorizer` dengan **n-gram (1, 2)** untuk menangkap konteks yang lebih baik.

### 4. Pemodelan dan Pelatihan
Beberapa algoritma digunakan dalam fase eksperimen:
* **Machine Learning Klasik**:
    * `LinearSVC`
    * `Logistic Regression`
    * *Hyperparameter tuning* dilakukan menggunakan `GridSearchCV` untuk mendapatkan performa optimal.
* **Deep Learning (Transformer)**:
    * *Fine-tuning* model **IndoBERT** (`indobenchmark/indobert-base-p1`) menggunakan *library* **Hugging Face Transformers** dan **PyTorch**.

---

## üìà **Hasil & Evaluasi**

Semua model yang dikembangkan berhasil melampaui target akurasi 85%. Model **IndoBERT** menunjukkan performa terbaik, membuktikan keunggulannya dalam memahami konteks Bahasa Indonesia.

| Model | Akurasi Training Set | Akurasi Testing Set | F1-Score (rata-rata) |
| :--- | :---: | :---: | :---: |
| **IndoBERT (Fine-Tuned)** | **95.03%** | **88.35%** | **0.88** |
| Logistic Regression | 89.21% | 87.00% | 0.87 |
| LinearSVC | 89.29% | 87.00% | 0.87 |
| SVM (Baseline) | 90.92% | 86.36% | 0.86 |

Model **IndoBERT** menjadi model final karena memberikan akurasi dan F1-Score tertinggi pada data uji.

---

## üíª **Teknologi yang Digunakan**

* **Bahasa & Lingkungan**: `Python`, `Google Colab`
* **Pengumpulan Data**: `google-play-scraper`
* **Analisis & Preprocessing**: `Pandas`, `NLTK`, `Sastrawi`
* **Pemodelan**: `Scikit-learn`, `Hugging Face Transformers`, `PyTorch`
* **Evaluasi**: `Evaluate` (dari Hugging Face)

---

## üöÄ **Cara Menjalankan Proyek**

1.  **Clone Repositori**:
    ```bash
    git clone [https://github.com/NAMA_USER/NAMA_REPO.git](https://github.com/NAMA_USER/NAMA_REPO.git)
    cd NAMA_REPO
    ```

2.  **Install Dependensi**:
    ```bash
    pip install -r requirements.txt
    ```

3.  **Jalankan Notebook**:
    * Buka dan jalankan file `Crawling_Data.ipynb` untuk mengumpulkan data terbaru.
    * Buka dan jalankan file `Train_Model.ipynb` untuk melalui proses preprocessing hingga pelatihan model.
